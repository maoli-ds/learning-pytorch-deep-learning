{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport sys\nimport string\nimport math\n!pip install pycocotools\n!pip install torchinfo\nfrom pycocotools.coco import COCO\nimport json","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-07-19T13:38:59.513735Z","iopub.execute_input":"2022-07-19T13:38:59.514108Z","iopub.status.idle":"2022-07-19T13:39:48.917083Z","shell.execute_reply.started":"2022-07-19T13:38:59.513970Z","shell.execute_reply":"2022-07-19T13:39:48.915752Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting pycocotools\n  Downloading pycocotools-2.0.4.tar.gz (106 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.6/106.6 kB\u001b[0m \u001b[31m468.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from pycocotools) (1.21.6)\nRequirement already satisfied: matplotlib>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from pycocotools) (3.5.2)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools) (1.4.2)\nRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools) (3.0.9)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools) (4.33.3)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools) (2.8.2)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools) (9.1.1)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools) (21.3)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib>=2.1.0->pycocotools) (4.1.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools) (1.16.0)\nBuilding wheels for collected packages: pycocotools\n  Building wheel for pycocotools (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pycocotools: filename=pycocotools-2.0.4-cp37-cp37m-linux_x86_64.whl size=370081 sha256=332f007f572e0f6e67e9d5739fb5953997e57ba13cffcdfb0897ce4a4135300b\n  Stored in directory: /root/.cache/pip/wheels/a3/5f/fa/f011e578cc76e1fc5be8dce30b3eb9fd00f337e744b3bba59b\nSuccessfully built pycocotools\nInstalling collected packages: pycocotools\nSuccessfully installed pycocotools-2.0.4\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting torchinfo\n  Downloading torchinfo-1.7.0-py3-none-any.whl (22 kB)\nInstalling collected packages: torchinfo\nSuccessfully installed torchinfo-1.7.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"folder1 = '../input/mscoco-cv/MSCOCO/MSCOCO'\nfolder2 = '../input/coco2014/captions'","metadata":{"execution":{"iopub.status.busy":"2022-07-19T13:39:48.920264Z","iopub.execute_input":"2022-07-19T13:39:48.920864Z","iopub.status.idle":"2022-07-19T13:39:48.927513Z","shell.execute_reply.started":"2022-07-19T13:39:48.920825Z","shell.execute_reply":"2022-07-19T13:39:48.926556Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"test_info = json.loads(open(os.path.join(folder1,'annotations','image_info_test2014.json')).read())\ntest_info.keys()","metadata":{"execution":{"iopub.status.busy":"2022-07-18T14:21:53.276898Z","iopub.execute_input":"2022-07-18T14:21:53.277205Z","iopub.status.idle":"2022-07-18T14:21:53.616516Z","shell.execute_reply.started":"2022-07-18T14:21:53.277177Z","shell.execute_reply":"2022-07-18T14:21:53.615407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataType = 'val2014'\ninstances_file = os.path.join(folder2, 'annotations','instances_{}.json'.format(dataType))\ncoco= COCO(instances_file)","metadata":{"execution":{"iopub.status.busy":"2022-07-18T14:21:53.617997Z","iopub.execute_input":"2022-07-18T14:21:53.618325Z","iopub.status.idle":"2022-07-18T14:22:04.138277Z","shell.execute_reply.started":"2022-07-18T14:21:53.618295Z","shell.execute_reply":"2022-07-18T14:22:04.136919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"coco.__dict__.keys()","metadata":{"execution":{"iopub.status.busy":"2022-07-18T14:22:04.140035Z","iopub.execute_input":"2022-07-18T14:22:04.140724Z","iopub.status.idle":"2022-07-18T14:22:04.149199Z","shell.execute_reply.started":"2022-07-18T14:22:04.140673Z","shell.execute_reply":"2022-07-18T14:22:04.148034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dir(coco)","metadata":{"execution":{"iopub.status.busy":"2022-07-18T14:22:04.154497Z","iopub.execute_input":"2022-07-18T14:22:04.154916Z","iopub.status.idle":"2022-07-18T14:22:04.173648Z","shell.execute_reply.started":"2022-07-18T14:22:04.154880Z","shell.execute_reply":"2022-07-18T14:22:04.172329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.listdir(os.path.join('../input/coco2014', 'val2014', 'val2014'))[:10]","metadata":{"execution":{"iopub.status.busy":"2022-07-18T14:22:04.175492Z","iopub.execute_input":"2022-07-18T14:22:04.176538Z","iopub.status.idle":"2022-07-18T14:22:09.068919Z","shell.execute_reply.started":"2022-07-18T14:22:04.176499Z","shell.execute_reply":"2022-07-18T14:22:09.067634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(list(coco.anns.keys())[:2])","metadata":{"execution":{"iopub.status.busy":"2022-07-18T14:22:09.070567Z","iopub.execute_input":"2022-07-18T14:22:09.071317Z","iopub.status.idle":"2022-07-18T14:22:09.096315Z","shell.execute_reply.started":"2022-07-18T14:22:09.071274Z","shell.execute_reply":"2022-07-18T14:22:09.095043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(list(coco.anns.values())[:2])","metadata":{"execution":{"iopub.status.busy":"2022-07-18T14:22:09.097609Z","iopub.execute_input":"2022-07-18T14:22:09.098717Z","iopub.status.idle":"2022-07-18T14:22:09.135495Z","shell.execute_reply.started":"2022-07-18T14:22:09.098678Z","shell.execute_reply":"2022-07-18T14:22:09.134296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"coco_caption = COCO(os.path.join(folder2, 'annotations','captions_{}.json'.format(dataType)))","metadata":{"execution":{"iopub.status.busy":"2022-07-18T14:22:09.137490Z","iopub.execute_input":"2022-07-18T14:22:09.138333Z","iopub.status.idle":"2022-07-18T14:22:10.409176Z","shell.execute_reply.started":"2022-07-18T14:22:09.138284Z","shell.execute_reply":"2022-07-18T14:22:10.407999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list(coco_caption.anns.keys())[0]","metadata":{"execution":{"iopub.status.busy":"2022-07-18T14:22:10.410563Z","iopub.execute_input":"2022-07-18T14:22:10.410943Z","iopub.status.idle":"2022-07-18T14:22:10.423941Z","shell.execute_reply.started":"2022-07-18T14:22:10.410911Z","shell.execute_reply":"2022-07-18T14:22:10.422359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"coco_caption.anns[37]","metadata":{"execution":{"iopub.status.busy":"2022-07-18T14:22:10.426572Z","iopub.execute_input":"2022-07-18T14:22:10.427127Z","iopub.status.idle":"2022-07-18T14:22:10.436086Z","shell.execute_reply.started":"2022-07-18T14:22:10.427076Z","shell.execute_reply":"2022-07-18T14:22:10.434843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import skimage.io as io\nimport matplotlib.pyplot as plt\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2022-07-19T13:39:48.929025Z","iopub.execute_input":"2022-07-19T13:39:48.930004Z","iopub.status.idle":"2022-07-19T13:39:49.371150Z","shell.execute_reply.started":"2022-07-19T13:39:48.929961Z","shell.execute_reply":"2022-07-19T13:39:49.365763Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"ids = list(coco.anns.keys())\nkey_id = np.random.choice(ids)\nimg_id = coco.anns[key_id]['image_id']\nimg_file = coco.loadImgs(img_id)[0]['file_name']\n\n# print URL and visualize corresponding image\nprint(img_file)\nI = io.imread(os.path.join('../input/coco2014', 'val2014', 'val2014', img_file))\nplt.axis('off')\nplt.imshow(I)\nplt.show()\n\n# load and display captions\nannIds = coco_caption.getAnnIds(imgIds=coco.loadImgs(img_id)[0]['id']);\nanns = coco_caption.loadAnns(annIds)\ncoco_caption.showAnns(anns)","metadata":{"execution":{"iopub.status.busy":"2022-07-18T14:22:10.951493Z","iopub.execute_input":"2022-07-18T14:22:10.951815Z","iopub.status.idle":"2022-07-18T14:22:11.303987Z","shell.execute_reply.started":"2022-07-18T14:22:10.951785Z","shell.execute_reply":"2022-07-18T14:22:11.303070Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Shape of image is {}'.format(I.shape))","metadata":{"execution":{"iopub.status.busy":"2022-07-18T14:22:11.305010Z","iopub.execute_input":"2022-07-18T14:22:11.305306Z","iopub.status.idle":"2022-07-18T14:22:11.311360Z","shell.execute_reply.started":"2022-07-18T14:22:11.305274Z","shell.execute_reply":"2022-07-18T14:22:11.310203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchvision import transforms\n\nimg_transform = transforms.Compose([ \n    transforms.Resize(256),                          # smaller edge of image resized to 256\n    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n    # for each color channel we pass 2 values: \n    # mean and std. deviation\n    # since out image has 3 color channels, we pass 3 values for mean and  values for std. deviation\n    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n                         (0.229, 0.224, 0.225))])\n","metadata":{"execution":{"iopub.status.busy":"2022-07-16T13:59:58.846161Z","iopub.execute_input":"2022-07-16T13:59:58.846740Z","iopub.status.idle":"2022-07-16T13:59:58.856303Z","shell.execute_reply.started":"2022-07-16T13:59:58.846704Z","shell.execute_reply":"2022-07-16T13:59:58.855323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nimport pickle\nimport json\nfrom collections import Counter\n\n","metadata":{"execution":{"iopub.status.busy":"2022-07-19T13:39:49.373941Z","iopub.execute_input":"2022-07-19T13:39:49.374586Z","iopub.status.idle":"2022-07-19T13:39:50.420523Z","shell.execute_reply.started":"2022-07-19T13:39:49.374543Z","shell.execute_reply":"2022-07-19T13:39:50.419452Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class Vocabulary(object):\n    def __init__(self, vocab_threshold, captions_file,\n                 start_word='<start>', end_word='<end>', unk_word='<unk>'):\n        self.vocab_threshold = vocab_threshold\n        self.start_word = start_word\n        self.end_word = end_word\n        self.unk_word = unk_word\n        self.annotations_file = captions_file\n        self.build_vocab()\n    def init_vocab(self):\n        \"\"\"Initialize the dictionaries for converting tokens to integers (and vice-versa).\"\"\"\n        self.word2idx = {}\n        self.idx2word = {}\n        self.idx = 0\n    def add_word(self, word):\n        \"\"\"Add a token to the vocabulary.\"\"\"\n        if not word in self.word2idx:\n            self.word2idx[word] = self.idx\n            self.idx2word[self.idx] = word\n            self.idx += 1\n    def add_captions(self):\n        \"\"\"Loop over training captions and add all tokens to the vocabulary that meet or exceed the threshold.\"\"\"\n        coco = COCO(self.annotations_file)\n        counter = Counter()\n        ids = coco.anns.keys()\n        for i, d in enumerate(ids):\n            caption = str(coco.anns[d]['caption'])\n            tokens = nltk.tokenize.word_tokenize(caption.lower())\n            counter.update(tokens)\n\n            if i % 100000 == 0:\n                print(\"[%d/%d] Tokenizing captions...\" % (i, len(ids)))\n\n        words = [word for word, cnt in counter.items() if cnt >= self.vocab_threshold]\n\n        for i, word in enumerate(words):\n            self.add_word(word)\n    def build_vocab(self):\n        \"\"\"Populate the dictionaries for converting tokens to integers (and vice-versa).\"\"\"\n        self.init_vocab()\n        self.add_word(self.start_word)\n        self.add_word(self.end_word)\n        self.add_word(self.unk_word)\n        self.add_captions()\n    def __call__(self, word):\n        if not word in self.word2idx:\n            return self.word2idx[self.unk_word]\n        return self.word2idx[word]\n    def __len__(self):\n        return len(self.word2idx)    ","metadata":{"execution":{"iopub.status.busy":"2022-07-19T13:39:50.422082Z","iopub.execute_input":"2022-07-19T13:39:50.422416Z","iopub.status.idle":"2022-07-19T13:39:50.436356Z","shell.execute_reply.started":"2022-07-19T13:39:50.422381Z","shell.execute_reply":"2022-07-19T13:39:50.435453Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.utils.data as data\nfrom tqdm import tqdm\nimport random\nfrom PIL import Image","metadata":{"execution":{"iopub.status.busy":"2022-07-19T13:40:15.633334Z","iopub.execute_input":"2022-07-19T13:40:15.633925Z","iopub.status.idle":"2022-07-19T13:40:15.638549Z","shell.execute_reply.started":"2022-07-19T13:40:15.633886Z","shell.execute_reply":"2022-07-19T13:40:15.637578Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class CoCoDataset(data.Dataset):\n    def __init__(self, transform, mode, batch_size, vocab_threshold, start_word, \n        end_word, unk_word, annotations_file, img_folder):\n        self.transform = transform\n        self.mode = mode\n        self.batch_size = batch_size\n        if self.mode!='test':\n            self.vocab = Vocabulary(vocab_threshold, annotations_file, start_word,\n            end_word, unk_word)\n        self.img_folder = img_folder\n        if self.mode == 'train' or self.mode == 'valid':\n            # JSON file, where the annotations are stored\n            self.coco = COCO(annotations_file) \n            # each annotatin contains multiple attributes, such as task e.g. segmentation,\n            # image_id, bounding box and etc.\n            # in order to load an image, for instance, \n            # image URL we will use self.coco.loadImgs(image_id) based on image id\n            self.ids = list(self.coco.anns.keys())\n            print('Obtaining caption lengths...')\n            # get all_tokens - a big list of lists. Each is a list of tokens for specific caption\n            all_tokens = [nltk.tokenize.word_tokenize(str(self.coco.anns[self.ids[index]]['caption']).lower()) for index in tqdm(np.arange(len(self.ids)))]\n            # list of token lengths (number of words for each caption)\n            self.caption_lengths = [len(token) for token in all_tokens]           \n        else:\n            # if we are in testing mode\n            test_info = json.loads(open(annotations_file).read())\n            self.paths = [item['file_name'] for item in test_info['images']]\n    def process_caption(self, anns):\n        tokens = nltk.tokenize.word_tokenize(str(anns).lower())\n        caption = []\n        caption.append(self.vocab(self.vocab.start_word))\n        caption.extend([self.vocab(token) for token in tokens])\n        caption.append(self.vocab(self.vocab.end_word))\n        caption = torch.Tensor(caption).long()\n        return caption\n    def __getitem__(self, index):\n        if self.mode=='train':\n            ann_id = self.ids[index]\n            caption = self.coco.anns[ann_id]['caption']\n            img_id = self.coco.anns[ann_id]['image_id']\n            path = self.coco.loadImgs(img_id)[0]['file_name']\n            image = Image.open(os.path.join(self.img_folder, path)).convert('RGB')\n            image = self.transform(image)\n            caption = self.process_caption(caption)\n            # return pre-processed image and caption tensors\n            # image pre-processed with tranformer applied\n            return image, caption\n        elif self.mode=='valid':\n            ann_id = self.ids[index]\n            # Convert image to tensor and pre-process using transform           \n            caption = self.coco.anns[ann_id]['caption']\n            img_id = self.coco.anns[ann_id]['image_id']\n            path = self.coco.loadImgs(img_id)[0]['file_name']\n            image = Image.open(os.path.join(self.img_folder, path)).convert('RGB')\n            image = self.transform(image)\n            caption = self.process_caption(caption)\n            caps_all = []\n            # based on image id, get id of all related annotations\n            ids_ann = self.coco.getAnnIds(imgIds=img_id)\n            for ann_id in ids_ann:\n                capt = self.coco.anns[ann_id]['caption']\n                caps_all.append(capt)            \n            # return original image and pre-processed image tensor\n            return image, caption, caps_all\n        else:\n            path = self.paths[index]\n            # Convert image to tensor and pre-process using transform\n            PIL_image = Image.open(os.path.join(self.img_folder, path)).convert('RGB')\n            orig_image = np.array(PIL_image)\n            image = self.transform(PIL_image)\n            # return original image and pre-processed image tensor\n            return orig_image, image\n    def get_indices(self):\n        # randomly select the caption length from the list of lengths\n        sel_length = np.random.choice(self.caption_lengths)\n        all_indices = np.where([self.caption_lengths[i] == sel_length for i in np.arange(len(self.caption_lengths))])[0]\n        # select m = batch_size captions from list above\n        indices = list(np.random.choice(all_indices, size=self.batch_size))\n        # return the caption indices of specified batch\n        return indices\n\n    def __len__(self):\n        if self.mode == 'train' or self.mode == 'valid':\n            return len(self.ids)\n        else:\n            return len(self.paths)         \n            \n                \n            ","metadata":{"execution":{"iopub.status.busy":"2022-07-19T13:40:17.967771Z","iopub.execute_input":"2022-07-19T13:40:17.968130Z","iopub.status.idle":"2022-07-19T13:40:17.993010Z","shell.execute_reply.started":"2022-07-19T13:40:17.968098Z","shell.execute_reply":"2022-07-19T13:40:17.991944Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def get_loader(transform,\n               mode='train',\n               # default batch size\n               batch_size=1,\n               vocab_threshold=None,\n               start_word=\"<start>\",\n               end_word=\"<end>\",\n               unk_word=\"<unk>\",\n               num_workers=0):\n    #path = os.getcwd()\n    path = folder2\n    # Based on mode (train, val, test), obtain img_folder and annotations_file.\n    img_path = '../input/coco2014'\n    if mode == 'train':\n        img_folder = os.path.join(img_path, 'train2014', 'train2014')\n        annotations_file = os.path.join(path, 'annotations', 'captions_train2014.json')\n        \n    elif mode == 'valid':\n        img_folder = os.path.join(img_path, 'val2014', 'val2014')\n        annotations_file = os.path.join(path, 'annotations', 'captions_val2014.json')\n        \n    elif mode == 'test':\n        img_folder = os.path.join(folder1, 'test2014')        \n        annotations_file = os.path.join(folder1, 'annotations', 'image_info_test2014.json')\n\n    # COCO caption dataset.\n    dataset = CoCoDataset(transform=transform,\n                          mode=mode,\n                          batch_size=batch_size,\n                          vocab_threshold=vocab_threshold,\n                          start_word=start_word,\n                          end_word=end_word,\n                          unk_word=unk_word,\n                          annotations_file=annotations_file,\n                          img_folder=img_folder)\n\n    if mode == 'train' or mode == 'valid':\n        # Randomly sample a caption length and indices of that length\n        indices = dataset.get_indices()\n        # Create and assign a batch sampler to retrieve a batch with the sampled indices\n        # functionality from torch.utils\n        initial_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n        data_loader = data.DataLoader(dataset=dataset, \n                                      num_workers=num_workers,\n                                      batch_sampler=data.sampler.BatchSampler(sampler=initial_sampler,\n                                                                              batch_size=dataset.batch_size,\n                                                                              drop_last=False))\n    elif mode == 'test':\n        data_loader = data.DataLoader(dataset=dataset,\n                                      batch_size=dataset.batch_size,\n                                      shuffle=True,\n                                      num_workers=num_workers)\n\n    return data_loader","metadata":{"execution":{"iopub.status.busy":"2022-07-19T13:40:29.891626Z","iopub.execute_input":"2022-07-19T13:40:29.892292Z","iopub.status.idle":"2022-07-19T13:40:29.904050Z","shell.execute_reply.started":"2022-07-19T13:40:29.892255Z","shell.execute_reply":"2022-07-19T13:40:29.902705Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"#vocab_threshold = 5\n\n\n#batch_size = 10\n\n# Obtain the data loader.\n#data_loader = get_loader(transform=img_transform,\n#                         mode='train',\n#                         batch_size=batch_size,\n#                         vocab_threshold=vocab_threshold)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print('The shape of first image:', data_loader.dataset[0][0].shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torchvision.models as models\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchinfo import summary\n\n#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nclass EncoderCNN(nn.Module):\n    \"\"\"Encoder inputs images and returns feature maps\"\"\"\n    def __init__(self):\n        super(EncoderCNN, self).__init__()\n        resnet = models.resnet152(pretrained=True)\n        for param in resnet.parameters():\n            param.requires_grad_(False)\n\n        modules = list(resnet.children())[:-2]\n        self.resnet = nn.Sequential(*modules)\n\n    def forward(self, images):\n        features = self.resnet(images)\n        # first, we need to resize the tensor to be\n        # (batch, size*size, feature_maps)\n        batch, feature_maps, size_1, size_2 = features.size()\n        features = features.permute(0, 2, 3, 1)\n        features = features.view(batch, size_1*size_2, feature_maps)\n\n        return features","metadata":{"execution":{"iopub.status.busy":"2022-07-19T13:40:38.962597Z","iopub.execute_input":"2022-07-19T13:40:38.963261Z","iopub.status.idle":"2022-07-19T13:40:39.190277Z","shell.execute_reply.started":"2022-07-19T13:40:38.963224Z","shell.execute_reply":"2022-07-19T13:40:39.189153Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class BahdanauAttention(nn.Module):\n    def __init__(self, num_features, hidden_dim, output_dim = 1):\n        super(BahdanauAttention, self).__init__()\n        self.num_features = num_features\n        self.hidden_dim = hidden_dim\n        self.output_dim = output_dim\n        self.W = nn.Linear(self.num_features, self.hidden_dim)\n        self.U = nn.Linear(self.hidden_dim, self.hidden_dim)\n        self.v = nn.Linear(self.hidden_dim, self.output_dim)\n    def forward(self, features, decoder_hidden):\n        decoder_hidden = decoder_hidden.unsqueeze(1)\n        atten_1 = self.W(features)\n        atten_2 = self.U(decoder_hidden)\n        atten_tan = torch.tanh(atten_1+atten_2)\n        atten_score = self.v(atten_tan)\n        atten_weight = F.softmax(atten_score, dim = 1)\n        context = torch.sum(atten_weight * features,  dim = 1)\n        atten_weight = atten_weight.squeeze(dim=2)\n        return context, atten_weight\n    \n        ","metadata":{"execution":{"iopub.status.busy":"2022-07-19T13:40:44.709287Z","iopub.execute_input":"2022-07-19T13:40:44.709756Z","iopub.status.idle":"2022-07-19T13:40:44.720954Z","shell.execute_reply.started":"2022-07-19T13:40:44.709714Z","shell.execute_reply":"2022-07-19T13:40:44.719976Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"class DecoderRNN(nn.Module):\n    \"\"\"Attributes:\n    - embedding_dim - specified size of embeddings;\n    - hidden_dim - the size of RNN layer (number of hidden states)\n    - vocab_size - size of vocabulary\n    - p - dropout probability\n    \"\"\"\n    def __init__(self, num_features, embedding_dim, hidden_dim, vocab_size, p=0.6):\n        super(DecoderRNN, self).__init__()\n\n        self.num_features = num_features\n        self.embedding_dim = embedding_dim\n        self.hidden_dim = hidden_dim\n        self.vocab_size = vocab_size\n        # scale the inputs to softmax\n        self.sample_temp = 0.5\n\n        # embedding layer that turns words into a vector of a specified size\n        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n        # LSTM will have a single layer of size 512 (512 hidden units)\n        # it will input concatinated context vector (produced by attention)\n        # and corresponding hidden state of Decoder\n        self.lstm = nn.LSTMCell(embedding_dim + num_features, hidden_dim)\n        # produce the final output\n        self.fc = nn.Linear(hidden_dim, vocab_size)\n\n        # add attention layer\n        self.attention = BahdanauAttention(num_features, hidden_dim)\n        # dropout layer\n        self.drop = nn.Dropout(p=p)\n        # add initialization fully-connected layers\n        # initialize hidden state and cell memory using average feature vector\n        # Source: https://arxiv.org/pdf/1502.03044.pdf\n        self.init_h = nn.Linear(num_features, hidden_dim)\n        self.init_c = nn.Linear(num_features, hidden_dim)\n\n    def forward(self, captions, features, sample_prob = 0.0):\n        \"\"\"Arguments\n        ----------\n        - captions - image captions\n        - features - features returned from Encoder\n        - sample_prob - use it for scheduled sampling\n        Returns\n        ----------\n        - outputs - output logits from t steps\n        - atten_weights - weights from attention network\n        \"\"\"\n        # create embeddings for captions of size (batch, sqe_len, embed_dim)\n        embed = self.embeddings(captions)\n        h, c = self.init_hidden(features)\n        seq_len = captions.size(1)\n        feature_size = features.size(1)\n        batch_size = features.size(0)\n        # these tensors will store the outputs from lstm cell and attention weights\n        outputs = torch.zeros(batch_size, seq_len, self.vocab_size).to(device)\n        atten_weights = torch.zeros(batch_size, seq_len, feature_size).to(device)\n\n        # scheduled sampling for training\n        # we do not use it at the first timestep (<start> word)\n        # but later we check if the probability is bigger than random\n        for t in range(seq_len):\n            sample_prob = 0.0 if t == 0 else 0.5\n            use_sampling = np.random.random() < sample_prob\n            if use_sampling == False:\n                word_embed = embed[:,t,:]\n            context, atten_weight = self.attention(features, h)\n            # input_concat shape at time step t = (batch, embedding_dim + hidden_dim)\n            input_concat = torch.cat([word_embed, context], 1)\n            h, c = self.lstm(input_concat, (h,c))\n            h = self.drop(h)\n            output = self.fc(h)\n            if use_sampling == True:\n                # use sampling temperature to amplify the values before applying softmax\n                scaled_output = output / self.sample_temp\n                scoring = F.log_softmax(scaled_output, dim=1)\n                top_idx = scoring.topk(1)[1]\n                word_embed = self.embeddings(top_idx).squeeze(1)\n            outputs[:, t, :] = output\n            atten_weights[:, t, :] = atten_weight\n        return outputs, atten_weights\n\n    def init_hidden(self, features):\n\n        \"\"\"Initializes hidden state and cell memory using average feature vector.\n        Arguments:\n        ----------\n        - features - features returned from Encoder\n        Retruns:\n        ----------\n        - h0 - initial hidden state (short-term memory)\n        - c0 - initial cell state (long-term memory)\n        \"\"\"\n        mean_annotations = torch.mean(features, dim = 1)\n        h0 = self.init_h(mean_annotations)\n        c0 = self.init_c(mean_annotations)\n        return h0, c0\n\n\n    def greedy_search(self, features, max_sentence = 20):\n\n        \"\"\"Greedy search to sample top candidate from distribution.\n        Arguments\n        ----------\n        - features - features returned from Encoder\n        - max_sentence - max number of token per caption (default=20)\n        Returns:\n        ----------\n        - sentence - list of tokens\n        \"\"\"\n\n        sentence = []\n        weights = []\n        input_word = torch.tensor(0).unsqueeze(0).to(device)\n        h, c = self.init_hidden(features)\n        while True:\n            embedded_word = self.embeddings(input_word)\n            context, atten_weight = self.attention(features, h)\n            # input_concat shape at time step t = (batch, embedding_dim + context size)\n            input_concat = torch.cat([embedded_word, context],  dim = 1)\n            h, c = self.lstm(input_concat, (h,c))\n            h = self.drop(h)\n            output = self.fc(h)\n            scoring = F.log_softmax(output, dim=1)\n            top_idx = scoring[0].topk(1)[1]\n            sentence.append(top_idx.item())\n            weights.append(atten_weight)\n            input_word = top_idx\n            if (len(sentence) >= max_sentence or top_idx == 1):\n                break\n        return sentence, weights\n    def beam_search(self, features, k=3, max_sequence=20):\n        input_word = torch.tensor(0).unsqueeze(0).to(device)\n        h, c = self.init_hidden(features)\n        sentence, weights = [], []\n        states = [(h,c)]*k\n        candidates = [[0, [input_word]]]*k\n        can_wts = [[]]*k\n        for j in range(0,max_sequence):\n            temp = []\n            temp_states = []\n            temp_wts = []\n            for i, can in enumerate(candidates):\n                input_word = can[-1][-1]\n                if input_word[0].cpu().detach().item()!=1:\n                    embedded_word = self.embeddings(input_word)\n                    context, atten_weight = self.attention(features, states[i][0])\n                    input_concat = torch.cat([embedded_word, context],  dim = 1)\n                    h, c = self.lstm(input_concat, states[i])\n                    h = self.drop(h)\n                    output = self.fc(h)\n                    scoring = F.log_softmax(output, dim=1)\n                    v, idx = scoring[0].topk(k)\n                    v, idx = list(v.cpu().detach().numpy()), list(idx.cpu().detach().numpy())\n                    for value, index in zip(v, idx):\n                        temp_wts.append(can_wts[i]+[atten_weight])\n                        temp_states.append((h,c))\n                        temp.append([value+can[0], can[-1]+[torch.tensor(index).unsqueeze(0).to(device)]])\n                else:\n                    temp.append(can)\n                    temp_states.append(states[i])\n                    temp_wts.append(can_wts[i])\n            new_states, new_candidates, new_wts = [], [], []\n            for count in range(0,k):\n                curr = max(temp, key=lambda x:x[0]/(len(x[1]))**0.75)\n                loc = temp.index(curr)\n                new_candidates.append(curr)\n                new_states.append(temp_states[loc])\n                new_wts.append(temp_wts[loc])\n                temp.pop(loc)\n                temp_states.pop(loc)\n                temp_wts.pop(loc)\n            candidates, states, can_wts = new_candidates, new_states, new_wts\n        sentence = [x.cpu().detach().item() for x in candidates[0][-1][1:]]   \n        weights = can_wts[0]\n        return sentence, weights\n            \n                \n                \n                \n                \n            \n            \n                \n                ","metadata":{"execution":{"iopub.status.busy":"2022-07-19T13:40:51.275628Z","iopub.execute_input":"2022-07-19T13:40:51.276048Z","iopub.status.idle":"2022-07-19T13:40:51.312955Z","shell.execute_reply.started":"2022-07-19T13:40:51.276014Z","shell.execute_reply":"2022-07-19T13:40:51.312078Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#indices = data_loader.dataset.get_indices()\n#print('sampled indices:', indices)\n\n# Create and assign a batch sampler to retrieve a batch with the sampled indices.\n#new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n#data_loader.batch_sampler.sampler = new_sampler\n    \n# Obtain the batch.\n#images, captions = next(iter(data_loader))\n    \n#print('images.shape:', images.shape)\n#print('captions.shape:', captions.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print(captions)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embed_size = 256\n\n#-#-#-# Do NOT modify the code below this line. #-#-#-#\n\n# Initialize the encoder. (Optional: Add additional arguments if necessary.)\nencoder = EncoderCNN()\n\n# Move the encoder to GPU if CUDA is available.\nencoder.to(device)\n    \n# Move last batch of images (from Step 2) to GPU if CUDA is available.   \nimages = images.to(device)\n\n# Pass the images through the encoder.\nfeatures = encoder(images)\n\nprint('type(features):', type(features))\nprint('features.shape:', features.shape)\n\n# Check that your encoder satisfies some requirements of the project! :D\nassert type(features)==torch.Tensor, \"Encoder output needs to be a PyTorch Tensor.\" \nassert (features.shape[0]==batch_size), \"The shape of the encoder output is incorrect.\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(summary(encoder))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Specify the number of features in the hidden state of the RNN decoder.\nhidden_size = 512\nnum_features = features.size(2)\n#-#-#-# Do NOT modify the code below this line. #-#-#-#\n\n# Store the size of the vocabulary.\nvocab_size = len(data_loader.dataset.vocab)\n\n# Initialize the decoder.\ndecoder = DecoderRNN( num_features = num_features,\n                     embedding_dim = embed_size,\n                     hidden_dim = hidden_size,\n                     vocab_size=vocab_size,\n                    )\n\n# Move the decoder to GPU if CUDA is available.\ndecoder.to(device)\n    \n# Move last batch of captions (from Step 1) to GPU if CUDA is available \n#captions = torch.tensor(captions).to(torch.int64)\ncaptions = captions.to(device)\n\n\n# Pass the encoder output and captions through the decoder.\noutputs = decoder(captions = captions,\n                  features = features)\n\nprint('Decoder outputs type:', type(outputs[0]))\nprint('Decoder outputs shape:', outputs[0].shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_hypothesis(terms_idx, data_loader):\n    \"\"\"Function outputs word tokens from output indices (terms_idx)\n    \"\"\"\n    hypothesis_list = []\n    vocab = data_loader.dataset.vocab.idx2word\n    for i in range(terms_idx.size(0)):\n        words = [vocab.get(idx.item()) for idx in terms_idx[i]]\n        words = [word for word in words if word not in (',', '.', '<end>')]\n        hypothesis_list.append(words)\n    return hypothesis_list\n\ndef punctuation_free(reference):\n    \"\"\"Function takes a caption and outputs punctuation free and lower cased caption\"\"\"\n    text = reference.split()\n    x = [''.join(c.lower() for c in s if c not in string.punctuation) for s in text]\n    return x\n\n\ndef get_batch_caps(caps_all, batch_size):\n    \"\"\"Function takes sampled captions for images and\n    returns punctuation-free preprocessed caps in batches\"\"\"\n    batch_caps_all = []\n    for batch_idx in range(batch_size):\n        batch_caps = [i for i in map(lambda t: (punctuation_free(t[batch_idx])), caps_all)]\n        batch_caps_all.append(batch_caps)\n    return batch_caps_all","metadata":{"execution":{"iopub.status.busy":"2022-07-19T13:41:15.483613Z","iopub.execute_input":"2022-07-19T13:41:15.484288Z","iopub.status.idle":"2022-07-19T13:41:15.495772Z","shell.execute_reply.started":"2022-07-19T13:41:15.484251Z","shell.execute_reply":"2022-07-19T13:41:15.494889Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"from nltk.translate.bleu_score import corpus_bleu","metadata":{"execution":{"iopub.status.busy":"2022-07-19T13:41:24.864652Z","iopub.execute_input":"2022-07-19T13:41:24.865263Z","iopub.status.idle":"2022-07-19T13:41:24.870581Z","shell.execute_reply.started":"2022-07-19T13:41:24.865226Z","shell.execute_reply":"2022-07-19T13:41:24.869191Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"## TODO #1: Select appropriate values for the Python variables below.\nbatch_size = 64          # batch size, change to 64\nvocab_threshold = 3        # minimum word count threshold\nvocab_from_file = True    # if True, load existing vocab file\nembed_size = 256           # dimensionality of image and word embeddings\nhidden_size = 512          # number of features in hidden state of the RNN decoder\nnum_features = 2048        # number of feature maps, produced by Encoder\nnum_epochs = 3             # number of training epochs\nsave_every = 1             # determines frequency of saving model weights\nprint_every = 200          # determines window for printing average loss\n\nlog_train = 'training_log.txt'       # name of files with saved training loss and perplexity\nlog_val = 'validation_log.txt'\nbleu = 'bleu.txt'","metadata":{"execution":{"iopub.status.busy":"2022-07-19T13:41:31.013259Z","iopub.execute_input":"2022-07-19T13:41:31.013720Z","iopub.status.idle":"2022-07-19T13:41:31.019513Z","shell.execute_reply.started":"2022-07-19T13:41:31.013682Z","shell.execute_reply":"2022-07-19T13:41:31.018591Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"from torchvision import transforms\n\n\ntransform_train = transforms.Compose([ \n    transforms.Resize(256),                          # smaller edge of image resized to 256\n    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n                         (0.229, 0.224, 0.225))])\n\n\n# Build data loader.\ndata_loader = get_loader(transform=transform_train,\n                         mode='train',\n                         batch_size=batch_size,\n                         vocab_threshold=vocab_threshold)\n\n# The size of the vocabulary.\nvocab_size = len(data_loader.dataset.vocab)\n\ntotal_step = math.ceil(len(data_loader.dataset.caption_lengths) / data_loader.batch_sampler.batch_size)\n\n\n\n\n# Setup the transforms\ntransform_test = transforms.Compose([ \n    transforms.Resize((224,224)),                   # smaller edge of image resized to 256\n    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n                         (0.229, 0.224, 0.225))])\n\n# Create the data loader.\nvalid_data_loader = get_loader(transform=transform_test,\n                                    batch_size=batch_size,\n                         mode='valid', vocab_threshold=vocab_threshold)\n\n\ntotal_step_valid = math.ceil(len(valid_data_loader.dataset.caption_lengths) / valid_data_loader.batch_sampler.batch_size)","metadata":{"execution":{"iopub.status.busy":"2022-07-19T13:41:39.851319Z","iopub.execute_input":"2022-07-19T13:41:39.851996Z","iopub.status.idle":"2022-07-19T13:44:53.735277Z","shell.execute_reply.started":"2022-07-19T13:41:39.851956Z","shell.execute_reply":"2022-07-19T13:44:53.734350Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"loading annotations into memory...\nDone (t=1.51s)\ncreating index...\nindex created!\n[0/414113] Tokenizing captions...\n[100000/414113] Tokenizing captions...\n[200000/414113] Tokenizing captions...\n[300000/414113] Tokenizing captions...\n[400000/414113] Tokenizing captions...\nloading annotations into memory...\nDone (t=0.84s)\ncreating index...\nindex created!\nObtaining caption lengths...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 414113/414113 [01:03<00:00, 6498.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"loading annotations into memory...\nDone (t=0.85s)\ncreating index...\nindex created!\n[0/202654] Tokenizing captions...\n[100000/202654] Tokenizing captions...\n[200000/202654] Tokenizing captions...\nloading annotations into memory...\nDone (t=0.40s)\ncreating index...\nindex created!\nObtaining caption lengths...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 202654/202654 [00:31<00:00, 6464.37it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Initialize the encoder and decoder. \nencoder = EncoderCNN()\ndecoder = DecoderRNN(num_features = num_features, \n                     embedding_dim = embed_size, \n                     hidden_dim = hidden_size, \n                     vocab_size = vocab_size)\n\n# Move models to GPU if CUDA is available. \ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nencoder.to(device)\ndecoder.to(device)\n\n\n# Define the loss function. \ncriterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n\n# TODO #3: Specify the learnable parameters of the model.\n#params = list(decoder.parameters()) + list(encoder.parameters()) \nparams = list(decoder.parameters())\n\n# TODO #4: Define the optimizer.\noptimizer = torch.optim.Adam(params, lr = 1e-4)","metadata":{"execution":{"iopub.status.busy":"2022-07-19T13:45:20.913915Z","iopub.execute_input":"2022-07-19T13:45:20.914272Z","iopub.status.idle":"2022-07-19T13:45:22.395485Z","shell.execute_reply.started":"2022-07-19T13:45:20.914237Z","shell.execute_reply":"2022-07-19T13:45:22.394541Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"out_folder = './'\n\ndef train(epoch, \n          encoder, \n          decoder, \n          optimizer, \n          criterion, total_step, num_epochs, data_loader, write_file, save_every = 1):\n    epoch_loss = 0.0\n    epoch_perplex = 0.0\n    \n    for i_step in range(1, total_step+1):\n        # training mode on\n        encoder.eval() # no fine-tuning for Encoder\n        decoder.train()\n        \n        # Randomly sample a caption length, and sample indices with that length.\n        #indices = data_loader.dataset.get_indices()\n        # Create and assign a batch sampler to retrieve a batch with the sampled indices.\n        #new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n        #data_loader.batch_sampler.sampler = new_sampler\n        \n        # Obtain the batch.\n        images, captions = next(iter(data_loader))\n        # target captions, excluding the first word\n        captions_target = captions[:, 1:].to(device) \n        # captions for training without the last word\n        captions_train = captions[:, :-1].to(device)\n\n        # Move batch of images and captions to GPU if CUDA is available.\n        images = images.to(device)\n        \n        # Zero the gradients.\n        decoder.zero_grad()\n        encoder.zero_grad()\n        \n        # Pass the inputs through the CNN-RNN model.\n        features = encoder(images)\n        outputs, atten_weights = decoder(captions= captions_train,\n                                         features = features)\n        \n        # Calculate the batch loss.\n        loss = criterion(outputs.view(-1, vocab_size), captions_target.reshape(-1))\n        \n        # Backward pass.\n        loss.backward()\n        \n        # Update the parameters in the optimizer.\n        optimizer.step()\n        \n        perplex = np.exp(loss.item())\n        epoch_loss += loss.item()\n        epoch_perplex += perplex\n        \n        stats = 'Epoch train: [%d/%d], Step train: [%d/%d], Loss train: %.4f, Perplexity train: %5.4f' % (epoch, num_epochs, i_step, total_step, loss.item(), perplex)\n        \n        # Print training statistics (on same line).\n        print('\\r' + stats, end=\"\")\n        sys.stdout.flush()\n        \n        # Print training statistics to file.\n        write_file.write(stats + '\\n')\n        write_file.flush()\n        \n        # Print training statistics (on different line).\n        if i_step % print_every == 0:\n            print('\\r' + stats)\n        \n    epoch_loss_avg = epoch_loss / total_step\n    epoch_perp_avg = epoch_perplex / total_step\n    \n    print('\\r')\n    print('Epoch train:', epoch)\n    print('\\r' + 'Avg. Loss train: %.4f, Avg. Perplexity train: %5.4f' % (epoch_loss_avg, epoch_perp_avg), end=\"\")\n    print('\\r')\n    \n    # Save the weights.\n    if epoch % save_every == 0:\n        parent_path = os.path.join('./','models_new')\n        if not os.path.exists(parent_path):\n            os.makedirs(parent_path)\n        torch.save(decoder.state_dict(), os.path.join(parent_path, 'decoder-%d.pkl' % epoch))\n        torch.save(encoder.state_dict(), os.path.join(parent_path, 'encoder-%d.pkl' % epoch))","metadata":{"execution":{"iopub.status.busy":"2022-07-19T13:45:22.397956Z","iopub.execute_input":"2022-07-19T13:45:22.398486Z","iopub.status.idle":"2022-07-19T13:45:22.413452Z","shell.execute_reply.started":"2022-07-19T13:45:22.398446Z","shell.execute_reply":"2022-07-19T13:45:22.412565Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def validate(epoch, \n             encoder, \n             decoder, \n             optimizer, \n             criterion, \n             total_step, num_epochs, data_loader, write_file, bleu_score_file):\n    epoch_loss = 0.0\n    epoch_perplex = 0.0\n    references = []\n    hypothesis = []\n    encoder.eval()\n    decoder.eval()\n    for i_step in range(1,total_step+1):\n        # evaluation of encoder and decoder\n        val_images, val_captions, caps_all = next(iter(data_loader))\n        val_captions_target = val_captions[:, 1:].to(device) \n        val_captions = val_captions[:, :-1].to(device)\n        val_images = val_images.to(device)\n        with torch.no_grad():\n            features_val = encoder(val_images)\n            outputs_val, atten_weights_val = decoder(captions= val_captions,\n                                         features = features_val)\n        loss_val = criterion(outputs_val.view(-1, vocab_size), \n                             val_captions_target.reshape(-1))\n        \n        # preprocess captions and add them to the list\n        caps_processed = get_batch_caps(caps_all, batch_size=batch_size)\n        references.append(caps_processed)\n        # get corresponding indicies from predictions\n        # and form hypothesis from output\n        terms_idx = torch.max(outputs_val, dim=2)[1]\n        hyp_list = get_hypothesis(terms_idx, data_loader=data_loader)\n        hypothesis.append(hyp_list)\n        \n        perplex = np.exp(loss_val.item())\n        epoch_loss += loss_val.item()\n        epoch_perplex += perplex\n        \n        stats = 'Epoch valid: [%d/%d], Step valid: [%d/%d], Loss valid: %.4f, Perplexity valid: %5.4f' % (epoch, num_epochs, i_step, total_step, loss_val.item(), perplex)\n        \n        # Print training statistics (on same line).\n        print('\\r' + stats, end=\"\")\n        sys.stdout.flush()\n        \n        # Print training statistics to file.\n        write_file.write(stats + '\\n')\n        write_file.flush()\n        \n        # Print training statistics (on different line).\n        if i_step % print_every == 0:\n            print('\\r' + stats)\n    \n    epoch_loss_avg = epoch_loss / total_step\n    epoch_perp_avg = epoch_perplex / total_step\n    # prepare the proper shape for computing BLEU scores\n    references = np.array(references).reshape(total_step*batch_size, -1)\n    #hyps = np.array(hypothesis).reshape(total_step*batch_size, -1)\n    hyps = np.concatenate(np.array(hypothesis))\n        \n    bleu_1 = corpus_bleu(references, hyps, weights = (1.0, 0, 0, 0))\n    bleu_2 = corpus_bleu(references, hyps, weights = (0.5, 0.5, 0, 0))\n    bleu_3 = corpus_bleu(references, hyps, weights = (1.0/3.0, 1.0/3.0, 1.0/3.0, 0))\n    bleu_4 = corpus_bleu(references, hyps, weights = (0.25, 0.25, 0.25, 0.25))\n    # append individual n_gram scores\n    #bleu_score_list.append((bleu_1, bleu_2, bleu_3, bleu_4))\n    \n    print('\\r')\n    print('Epoch valid:', epoch)\n    epoch_stat = 'Avg. Loss valid: %.4f, Avg. Perplexity valid: %5.4f, \\\n    BLEU-1: %.2f, BLEU-2: %.2f, BLEU-3: %.2f, BLEU-4: %.2f' % (epoch_loss_avg, epoch_perp_avg, bleu_1, bleu_2, bleu_3, bleu_4)\n    \n    print('\\r' + epoch_stat, end=\"\")\n    print('\\r')\n    \n    bleu_score_file.write(epoch_stat + '\\n')\n    bleu_score_file.flush()","metadata":{"execution":{"iopub.status.busy":"2022-07-19T13:45:22.414961Z","iopub.execute_input":"2022-07-19T13:45:22.415310Z","iopub.status.idle":"2022-07-19T13:45:22.433969Z","shell.execute_reply.started":"2022-07-19T13:45:22.415272Z","shell.execute_reply":"2022-07-19T13:45:22.432962Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# Open the training log file.\nfile_train = open(log_train, 'w')\nfile_val = open(log_val, 'w')\nbleu_score_file = open(bleu, 'w')\n\n\n# store BLEU scores in list \nbleu_scores = []\ntotal_step_valid = math.ceil(len(valid_data_loader.dataset.caption_lengths) / valid_data_loader.batch_sampler.batch_size)\n\nfor epoch in range(1, num_epochs+1):   \n    train(epoch, encoder, decoder, optimizer, criterion, total_step, num_epochs =num_epochs,\n          data_loader = data_loader,\n          write_file = file_train, \n          save_every = 1)\n    \n    validate(epoch, encoder, decoder, optimizer, criterion, \n             total_step = total_step_valid, \n             num_epochs = num_epochs, \n             data_loader = valid_data_loader, write_file=file_val, bleu_score_file=bleu_score_file)\n    \nfile_train.close()\nfile_val.close()\nbleu_score_file.close()","metadata":{"execution":{"iopub.status.busy":"2022-07-19T13:45:22.435249Z","iopub.execute_input":"2022-07-19T13:45:22.436219Z","iopub.status.idle":"2022-07-19T21:57:05.918119Z","shell.execute_reply.started":"2022-07-19T13:45:22.436190Z","shell.execute_reply":"2022-07-19T21:57:05.916906Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Epoch train: [1/3], Step train: [200/6471], Loss train: 2.1059, Perplexity train: 8.214260\nEpoch train: [1/3], Step train: [400/6471], Loss train: 0.9970, Perplexity train: 2.7100\nEpoch train: [1/3], Step train: [600/6471], Loss train: 0.6454, Perplexity train: 1.9068\nEpoch train: [1/3], Step train: [800/6471], Loss train: 0.4605, Perplexity train: 1.5849\nEpoch train: [1/3], Step train: [1000/6471], Loss train: 0.2642, Perplexity train: 1.3023\nEpoch train: [1/3], Step train: [1200/6471], Loss train: 0.2001, Perplexity train: 1.2215\nEpoch train: [1/3], Step train: [1400/6471], Loss train: 0.1753, Perplexity train: 1.1916\nEpoch train: [1/3], Step train: [1600/6471], Loss train: 0.1299, Perplexity train: 1.1388\nEpoch train: [1/3], Step train: [1800/6471], Loss train: 0.0865, Perplexity train: 1.0903\nEpoch train: [1/3], Step train: [2000/6471], Loss train: 0.0685, Perplexity train: 1.0709\nEpoch train: [1/3], Step train: [2200/6471], Loss train: 0.0594, Perplexity train: 1.0612\nEpoch train: [1/3], Step train: [2400/6471], Loss train: 0.0517, Perplexity train: 1.0531\nEpoch train: [1/3], Step train: [2600/6471], Loss train: 0.0435, Perplexity train: 1.0445\nEpoch train: [1/3], Step train: [2800/6471], Loss train: 0.0341, Perplexity train: 1.0346\nEpoch train: [1/3], Step train: [3000/6471], Loss train: 0.0237, Perplexity train: 1.0240\nEpoch train: [1/3], Step train: [3200/6471], Loss train: 0.0154, Perplexity train: 1.0156\nEpoch train: [1/3], Step train: [3400/6471], Loss train: 0.0108, Perplexity train: 1.0109\nEpoch train: [1/3], Step train: [3600/6471], Loss train: 0.0145, Perplexity train: 1.0146\nEpoch train: [1/3], Step train: [3800/6471], Loss train: 0.0119, Perplexity train: 1.0120\nEpoch train: [1/3], Step train: [4000/6471], Loss train: 0.0130, Perplexity train: 1.0131\nEpoch train: [1/3], Step train: [4200/6471], Loss train: 0.0093, Perplexity train: 1.0094\nEpoch train: [1/3], Step train: [4400/6471], Loss train: 0.0055, Perplexity train: 1.0055\nEpoch train: [1/3], Step train: [4600/6471], Loss train: 0.0074, Perplexity train: 1.0074\nEpoch train: [1/3], Step train: [4800/6471], Loss train: 0.0049, Perplexity train: 1.0049\nEpoch train: [1/3], Step train: [5000/6471], Loss train: 0.0042, Perplexity train: 1.0042\nEpoch train: [1/3], Step train: [5200/6471], Loss train: 0.0043, Perplexity train: 1.0044\nEpoch train: [1/3], Step train: [5400/6471], Loss train: 0.0033, Perplexity train: 1.0033\nEpoch train: [1/3], Step train: [5600/6471], Loss train: 0.0040, Perplexity train: 1.0040\nEpoch train: [1/3], Step train: [5800/6471], Loss train: 0.0030, Perplexity train: 1.0030\nEpoch train: [1/3], Step train: [6000/6471], Loss train: 0.0055, Perplexity train: 1.0055\nEpoch train: [1/3], Step train: [6200/6471], Loss train: 0.0025, Perplexity train: 1.0025\nEpoch train: [1/3], Step train: [6400/6471], Loss train: 0.0017, Perplexity train: 1.0017\nEpoch train: [1/3], Step train: [6471/6471], Loss train: 0.0023, Perplexity train: 1.0023\nEpoch train: 1\nAvg. Loss train: 0.2538, Avg. Perplexity train: 17.7206\nEpoch valid: [1/3], Step valid: [200/3167], Loss valid: 12.2196, Perplexity valid: 202728.6300\nEpoch valid: [1/3], Step valid: [400/3167], Loss valid: 12.0016, Perplexity valid: 163013.8962\nEpoch valid: [1/3], Step valid: [600/3167], Loss valid: 12.2109, Perplexity valid: 200973.6164\nEpoch valid: [1/3], Step valid: [800/3167], Loss valid: 12.0336, Perplexity valid: 168312.5454\nEpoch valid: [1/3], Step valid: [1000/3167], Loss valid: 12.0371, Perplexity valid: 168906.2126\nEpoch valid: [1/3], Step valid: [1200/3167], Loss valid: 11.8355, Perplexity valid: 138061.1635\nEpoch valid: [1/3], Step valid: [1400/3167], Loss valid: 12.1200, Perplexity valid: 183510.3941\nEpoch valid: [1/3], Step valid: [1600/3167], Loss valid: 12.2407, Perplexity valid: 207050.6734\nEpoch valid: [1/3], Step valid: [1800/3167], Loss valid: 12.0275, Perplexity valid: 167297.6293\nEpoch valid: [1/3], Step valid: [2000/3167], Loss valid: 12.1750, Perplexity valid: 193885.1343\nEpoch valid: [1/3], Step valid: [2200/3167], Loss valid: 11.8877, Perplexity valid: 145462.8838\nEpoch valid: [1/3], Step valid: [2400/3167], Loss valid: 12.0269, Perplexity valid: 167198.8986\nEpoch valid: [1/3], Step valid: [2600/3167], Loss valid: 12.0820, Perplexity valid: 176665.7905\nEpoch valid: [1/3], Step valid: [2800/3167], Loss valid: 12.1441, Perplexity valid: 187979.9623\nEpoch valid: [1/3], Step valid: [3000/3167], Loss valid: 12.2196, Perplexity valid: 202728.8233\nEpoch valid: [1/3], Step valid: [3167/3167], Loss valid: 11.9122, Perplexity valid: 149081.5292","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:56: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:58: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n/opt/conda/lib/python3.7/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \nCorpus/Sentence contains 0 counts of 3-gram overlaps.\nBLEU scores might be undesirable; use SmoothingFunction().\n  warnings.warn(_msg)\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch valid: 1\nAvg. Loss valid: 12.0485, Avg. Perplexity valid: 172025.1995,     BLEU-1: 0.32, BLEU-2: 0.01, BLEU-3: 0.06, BLEU-4: 0.12\nEpoch train: [2/3], Step train: [200/6471], Loss train: 0.0016, Perplexity train: 1.0016\nEpoch train: [2/3], Step train: [400/6471], Loss train: 0.0015, Perplexity train: 1.0015\nEpoch train: [2/3], Step train: [600/6471], Loss train: 0.0012, Perplexity train: 1.0012\nEpoch train: [2/3], Step train: [800/6471], Loss train: 0.0011, Perplexity train: 1.0011\nEpoch train: [2/3], Step train: [1000/6471], Loss train: 0.0011, Perplexity train: 1.0011\nEpoch train: [2/3], Step train: [1200/6471], Loss train: 0.0045, Perplexity train: 1.0045\nEpoch train: [2/3], Step train: [1400/6471], Loss train: 0.0015, Perplexity train: 1.0015\nEpoch train: [2/3], Step train: [1600/6471], Loss train: 0.0013, Perplexity train: 1.0013\nEpoch train: [2/3], Step train: [1800/6471], Loss train: 0.0016, Perplexity train: 1.0016\nEpoch train: [2/3], Step train: [2000/6471], Loss train: 0.0010, Perplexity train: 1.0010\nEpoch train: [2/3], Step train: [2200/6471], Loss train: 0.0006, Perplexity train: 1.0006\nEpoch train: [2/3], Step train: [2400/6471], Loss train: 0.0018, Perplexity train: 1.0018\nEpoch train: [2/3], Step train: [2600/6471], Loss train: 0.0007, Perplexity train: 1.0007\nEpoch train: [2/3], Step train: [2800/6471], Loss train: 0.0006, Perplexity train: 1.0006\nEpoch train: [2/3], Step train: [3000/6471], Loss train: 0.0004, Perplexity train: 1.0004\nEpoch train: [2/3], Step train: [3200/6471], Loss train: 0.0004, Perplexity train: 1.0004\nEpoch train: [2/3], Step train: [3400/6471], Loss train: 0.0005, Perplexity train: 1.0005\nEpoch train: [2/3], Step train: [3600/6471], Loss train: 0.0005, Perplexity train: 1.0005\nEpoch train: [2/3], Step train: [3800/6471], Loss train: 0.0003, Perplexity train: 1.0003\nEpoch train: [2/3], Step train: [4000/6471], Loss train: 0.0002, Perplexity train: 1.0002\nEpoch train: [2/3], Step train: [4200/6471], Loss train: 0.0003, Perplexity train: 1.0003\nEpoch train: [2/3], Step train: [4400/6471], Loss train: 0.0003, Perplexity train: 1.0003\nEpoch train: [2/3], Step train: [4600/6471], Loss train: 0.0002, Perplexity train: 1.0002\nEpoch train: [2/3], Step train: [4800/6471], Loss train: 0.0005, Perplexity train: 1.0005\nEpoch train: [2/3], Step train: [5000/6471], Loss train: 0.0002, Perplexity train: 1.0002\nEpoch train: [2/3], Step train: [5200/6471], Loss train: 0.0002, Perplexity train: 1.0002\nEpoch train: [2/3], Step train: [5400/6471], Loss train: 0.0001, Perplexity train: 1.0001\nEpoch train: [2/3], Step train: [5600/6471], Loss train: 0.0002, Perplexity train: 1.0002\nEpoch train: [2/3], Step train: [5800/6471], Loss train: 0.0014, Perplexity train: 1.0014\nEpoch train: [2/3], Step train: [6000/6471], Loss train: 0.0007, Perplexity train: 1.0007\nEpoch train: [2/3], Step train: [6200/6471], Loss train: 0.0004, Perplexity train: 1.0004\nEpoch train: [2/3], Step train: [6400/6471], Loss train: 0.0004, Perplexity train: 1.0004\nEpoch train: [2/3], Step train: [6471/6471], Loss train: 0.0005, Perplexity train: 1.0005\nEpoch train: 2\nAvg. Loss train: 0.0019, Avg. Perplexity train: 1.0021\nEpoch valid: [2/3], Step valid: [200/3167], Loss valid: 13.0539, Perplexity valid: 466927.9689\nEpoch valid: [2/3], Step valid: [400/3167], Loss valid: 13.1401, Perplexity valid: 508968.0551\nEpoch valid: [2/3], Step valid: [600/3167], Loss valid: 13.1911, Perplexity valid: 535552.0647\nEpoch valid: [2/3], Step valid: [800/3167], Loss valid: 12.7695, Perplexity valid: 351350.9241\nEpoch valid: [2/3], Step valid: [1000/3167], Loss valid: 12.9029, Perplexity valid: 401488.0790\nEpoch valid: [2/3], Step valid: [1200/3167], Loss valid: 12.9581, Perplexity valid: 424244.4211\nEpoch valid: [2/3], Step valid: [1400/3167], Loss valid: 13.1144, Perplexity valid: 496017.9949\nEpoch valid: [2/3], Step valid: [1600/3167], Loss valid: 12.8185, Perplexity valid: 368974.3465\nEpoch valid: [2/3], Step valid: [1800/3167], Loss valid: 12.8680, Perplexity valid: 387718.6848\nEpoch valid: [2/3], Step valid: [2000/3167], Loss valid: 12.8035, Perplexity valid: 363475.7098\nEpoch valid: [2/3], Step valid: [2200/3167], Loss valid: 12.8952, Perplexity valid: 398385.7093\nEpoch valid: [2/3], Step valid: [2400/3167], Loss valid: 12.9618, Perplexity valid: 425815.1117\nEpoch valid: [2/3], Step valid: [2600/3167], Loss valid: 12.8387, Perplexity valid: 376523.0804\nEpoch valid: [2/3], Step valid: [2800/3167], Loss valid: 12.9892, Perplexity valid: 437664.6345\nEpoch valid: [2/3], Step valid: [3000/3167], Loss valid: 13.2191, Perplexity valid: 550777.2235\nEpoch valid: [2/3], Step valid: [3167/3167], Loss valid: 12.7912, Perplexity valid: 359032.5073\nEpoch valid: 2\nAvg. Loss valid: 12.9398, Avg. Perplexity valid: 422927.4850,     BLEU-1: 0.33, BLEU-2: 0.01, BLEU-3: 0.06, BLEU-4: 0.12\nEpoch train: [3/3], Step train: [200/6471], Loss train: 0.0003, Perplexity train: 1.0003\nEpoch train: [3/3], Step train: [400/6471], Loss train: 0.0003, Perplexity train: 1.0003\nEpoch train: [3/3], Step train: [600/6471], Loss train: 0.0003, Perplexity train: 1.0003\nEpoch train: [3/3], Step train: [800/6471], Loss train: 0.0002, Perplexity train: 1.0002\nEpoch train: [3/3], Step train: [1000/6471], Loss train: 0.0002, Perplexity train: 1.0002\nEpoch train: [3/3], Step train: [1200/6471], Loss train: 0.0002, Perplexity train: 1.0002\nEpoch train: [3/3], Step train: [1400/6471], Loss train: 0.0003, Perplexity train: 1.0003\nEpoch train: [3/3], Step train: [1600/6471], Loss train: 0.0003, Perplexity train: 1.0003\nEpoch train: [3/3], Step train: [1800/6471], Loss train: 0.0002, Perplexity train: 1.0002\nEpoch train: [3/3], Step train: [2000/6471], Loss train: 0.0001, Perplexity train: 1.0001\nEpoch train: [3/3], Step train: [2200/6471], Loss train: 0.0001, Perplexity train: 1.0001\nEpoch train: [3/3], Step train: [2400/6471], Loss train: 0.0001, Perplexity train: 1.0001\nEpoch train: [3/3], Step train: [2600/6471], Loss train: 0.0001, Perplexity train: 1.0001\nEpoch train: [3/3], Step train: [2800/6471], Loss train: 0.0001, Perplexity train: 1.0001\nEpoch train: [3/3], Step train: [3000/6471], Loss train: 0.0001, Perplexity train: 1.0001\nEpoch train: [3/3], Step train: [3200/6471], Loss train: 0.0001, Perplexity train: 1.0001\nEpoch train: [3/3], Step train: [3400/6471], Loss train: 0.0001, Perplexity train: 1.0001\nEpoch train: [3/3], Step train: [3600/6471], Loss train: 0.0001, Perplexity train: 1.0001\nEpoch train: [3/3], Step train: [3800/6471], Loss train: 0.0001, Perplexity train: 1.0001\nEpoch train: [3/3], Step train: [4000/6471], Loss train: 0.0001, Perplexity train: 1.0001\nEpoch train: [3/3], Step train: [4200/6471], Loss train: 0.0001, Perplexity train: 1.0001\nEpoch train: [3/3], Step train: [4400/6471], Loss train: 0.0001, Perplexity train: 1.0001\nEpoch train: [3/3], Step train: [4600/6471], Loss train: 0.0000, Perplexity train: 1.0000\nEpoch train: [3/3], Step train: [4800/6471], Loss train: 0.0000, Perplexity train: 1.0000\nEpoch train: [3/3], Step train: [5000/6471], Loss train: 0.0000, Perplexity train: 1.0000\nEpoch train: [3/3], Step train: [5200/6471], Loss train: 0.0000, Perplexity train: 1.0000\nEpoch train: [3/3], Step train: [5400/6471], Loss train: 0.0000, Perplexity train: 1.0000\nEpoch train: [3/3], Step train: [5600/6471], Loss train: 0.0001, Perplexity train: 1.0001\nEpoch train: [3/3], Step train: [5800/6471], Loss train: 0.0002, Perplexity train: 1.0002\nEpoch train: [3/3], Step train: [6000/6471], Loss train: 0.0000, Perplexity train: 1.0000\nEpoch train: [3/3], Step train: [6200/6471], Loss train: 0.0000, Perplexity train: 1.0000\nEpoch train: [3/3], Step train: [6400/6471], Loss train: 0.0000, Perplexity train: 1.0000\nEpoch train: [3/3], Step train: [6471/6471], Loss train: 0.0000, Perplexity train: 1.0000\nEpoch train: 3\nAvg. Loss train: 0.0001, Avg. Perplexity train: 1.0001\nEpoch valid: [3/3], Step valid: [200/3167], Loss valid: 14.3399, Perplexity valid: 1689509.2400\nEpoch valid: [3/3], Step valid: [400/3167], Loss valid: 14.8903, Perplexity valid: 2929489.9804\nEpoch valid: [3/3], Step valid: [600/3167], Loss valid: 14.5064, Perplexity valid: 1995575.4717\nEpoch valid: [3/3], Step valid: [800/3167], Loss valid: 14.4541, Perplexity valid: 1893790.7612\nEpoch valid: [3/3], Step valid: [1000/3167], Loss valid: 14.2937, Perplexity valid: 1613072.9475\nEpoch valid: [3/3], Step valid: [1200/3167], Loss valid: 14.3162, Perplexity valid: 1649863.5036\nEpoch valid: [3/3], Step valid: [1400/3167], Loss valid: 14.5064, Perplexity valid: 1995575.4717\nEpoch valid: [3/3], Step valid: [1600/3167], Loss valid: 14.3610, Perplexity valid: 1725406.7162\nEpoch valid: [3/3], Step valid: [1800/3167], Loss valid: 14.3445, Perplexity valid: 1697194.5655\nEpoch valid: [3/3], Step valid: [2000/3167], Loss valid: 14.5428, Perplexity valid: 2069532.5702\nEpoch valid: [3/3], Step valid: [2200/3167], Loss valid: 14.3856, Perplexity valid: 1768341.0520\nEpoch valid: [3/3], Step valid: [2400/3167], Loss valid: 14.0122, Perplexity valid: 1217388.4108\nEpoch valid: [3/3], Step valid: [2600/3167], Loss valid: 14.3313, Perplexity valid: 1674943.9250\nEpoch valid: [3/3], Step valid: [2800/3167], Loss valid: 14.5502, Perplexity valid: 2084865.2179\nEpoch valid: [3/3], Step valid: [3000/3167], Loss valid: 14.1616, Perplexity valid: 1413492.8019\nEpoch valid: [3/3], Step valid: [3167/3167], Loss valid: 14.9801, Perplexity valid: 3204523.3894\nEpoch valid: 3\nAvg. Loss valid: 14.3795, Avg. Perplexity valid: 1812964.0716,     BLEU-1: 0.32, BLEU-2: 0.02, BLEU-3: 0.06, BLEU-4: 0.13\n","output_type":"stream"}]},{"cell_type":"code","source":"transform_test = transforms.Compose([ \n    transforms.Resize((224,224)),                          # smaller edge of image resized to 256\n    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n                         (0.229, 0.224, 0.225))])","metadata":{"execution":{"iopub.status.busy":"2022-07-18T14:30:25.347346Z","iopub.status.idle":"2022-07-18T14:30:25.348384Z","shell.execute_reply.started":"2022-07-18T14:30:25.348031Z","shell.execute_reply":"2022-07-18T14:30:25.348065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data_loader = get_loader(transform=transform_test,\n                                    batch_size=1,\n                         mode='test')","metadata":{"execution":{"iopub.status.busy":"2022-07-18T14:30:25.350585Z","iopub.status.idle":"2022-07-18T14:30:25.352061Z","shell.execute_reply.started":"2022-07-18T14:30:25.351694Z","shell.execute_reply":"2022-07-18T14:30:25.351738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab_size = len(test_data_loader.dataset.vocab)\nvocab_size","metadata":{"execution":{"iopub.status.busy":"2022-07-18T14:30:25.353848Z","iopub.status.idle":"2022-07-18T14:30:25.355282Z","shell.execute_reply.started":"2022-07-18T14:30:25.354914Z","shell.execute_reply":"2022-07-18T14:30:25.354949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoder_file = 'encoder-3.pkl' \ndecoder_file = 'decoder-3.pkl'\n\n\nembed_size = 256           # dimensionality of image and word embeddings\nhidden_size = 512          # number of features in hidden state of the RNN decoder\nnum_features = 2048        # number of feature maps, produced by Encoder\n\n# The size of the vocabulary.\nvocab_size = len(test_data_loader.dataset.vocab)\n\n# Initialize the encoder and decoder, and set each to inference mode.\nencoder = EncoderCNN()\nencoder.eval()\ndecoder = DecoderRNN(num_features = num_features, \n                     embedding_dim = embed_size, \n                     hidden_dim = hidden_size, \n                     vocab_size = vocab_size)\ndecoder.eval()\n\n\n# Load the trained weights.\nencoder.load_state_dict(torch.load(os.path.join('./‘，’models_new', encoder_file), map_location='cpu'))\ndecoder.load_state_dict(torch.load(os.path.join('./‘，‘models_new', decoder_file), map_location='cpu'))\n，\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Move models to GPU if CUDA is available.\nencoder.to(device)\ndecoder.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-07-18T14:30:25.357214Z","iopub.status.idle":"2022-07-18T14:30:25.358560Z","shell.execute_reply.started":"2022-07-18T14:30:25.358226Z","shell.execute_reply":"2022-07-18T14:30:25.358267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"orig_image, image = next(iter(test_data_loader))\nprint('Image shape:', list(image.size()))\nprint('Original image shape:', list(orig_image.size()))","metadata":{"execution":{"iopub.status.busy":"2022-07-18T14:30:25.360185Z","iopub.status.idle":"2022-07-18T14:30:25.361122Z","shell.execute_reply.started":"2022-07-18T14:30:25.360807Z","shell.execute_reply":"2022-07-18T14:30:25.360840Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_sentence(output, data_loader):\n    vocab = data_loader.dataset.vocab.idx2word\n    words = [vocab.get(idx) for idx in output]\n    words = [word for word in words if word not in (',', '.', '<end>')]\n    sentence = \" \".join(words)\n    return sentence","metadata":{"execution":{"iopub.status.busy":"2022-07-18T14:30:25.362988Z","iopub.status.idle":"2022-07-18T14:30:25.363599Z","shell.execute_reply.started":"2022-07-18T14:30:25.363297Z","shell.execute_reply":"2022-07-18T14:30:25.363326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_prediction(data_loader):\n    orig_image, image = next(iter(data_loader))\n    plt.imshow(orig_image.squeeze())\n    plt.title('Sample Image')\n    plt.show()\n    image = image.to(device)\n    features = encoder(image)\n    #output = decoder.sample(features)    \n    output, att_weights = decoder.greedy_search(features)\n    sentence = clean_sentence(output, data_loader)\n    print(sentence)","metadata":{"execution":{"iopub.status.busy":"2022-07-18T14:30:25.365187Z","iopub.status.idle":"2022-07-18T14:30:25.365610Z","shell.execute_reply.started":"2022-07-18T14:30:25.365417Z","shell.execute_reply":"2022-07-18T14:30:25.365436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(np.squeeze(orig_image))\nplt.title('example image')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-18T14:30:25.367300Z","iopub.status.idle":"2022-07-18T14:30:25.367691Z","shell.execute_reply.started":"2022-07-18T14:30:25.367507Z","shell.execute_reply":"2022-07-18T14:30:25.367524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image = image.to(device)\n\n# Obtain the embedded image features.\nprint(image.shape)\nfeatures = encoder(image)\n\n# Pass the embedded image features through the model to get a predicted caption.\noutput, att_weights = decoder.greedy_search(features)\nsentence = clean_sentence(output, test_data_loader)\nprint(sentence)","metadata":{"execution":{"iopub.status.busy":"2022-07-18T14:30:25.369015Z","iopub.status.idle":"2022-07-18T14:30:25.369905Z","shell.execute_reply.started":"2022-07-18T14:30:25.369659Z","shell.execute_reply":"2022-07-18T14:30:25.369681Z"},"trusted":true},"execution_count":null,"outputs":[]}]}